zookeeper:
    image: oddpoet/zookeeper
    hostname: zookeeper
    command:
        - "2181"
    ports:
        - "2181:2181"
kafka:
    # image: wurstmeister/kafka:0.8.2.1
    # image: wurstmeister/kafka:0.9.0.1
    image: wurstmeister/kafka:0.10.1.0-1
    hostname: kafka
    ports:
        - "9092:9092"
    expose:
        - "9092"
    links:
        - zookeeper:zk
    environment:
        KAFKA_ADVERTISED_PORT: 9092
        # KAFKA_ADVERTISED_HOST_NAME: localhost
        KAFKA_CREATE_TOPICS: "citizensensing.views:1:1,citizensensing.views.reply:1:1,devices:1:1,devices.reply:1:1,devices.data.traffic.count1:1:1,panels.vizbuilder:1:1,panels.vizbuilder.reply:1:1,datasets.tablesummariesquery:1:1,datasets.tablesummariesquery.reply:1:1,datasets.createcsvwfromcsvcmd:1:1,datasets.createcsvwfromcsvcmd.reply:1:1,datasets.createcsvwfromcsvcmd.reply:1:1,traffic.vehicle.1:1:1,traffic.count.1:1:1,traffic.count.2:1:1,http_datasets:1:1,http_datasets_reply:1:1,http_panels:1:1,http_panels_reply:1:1,datafile.created:1:1,datafile.schema.created:1:1,dataworkflowjobstart:1:1,dataworkflowjobstatus:1:1"
fuseki:
    image: stain/jena-fuseki
    ports:
        - "3030:3030"
    environment:
        ADMIN_PASSWORD: pw123
    # ToDo: Create new Docker image which creates a directory for dataset under /fuseki (?)
    command: /jena-fuseki/fuseki-server --update --loc=/fuseki /dutmp
redis:
    image: redis:2.8.9
filecache:
    build: ../dataunity-filecache
    volumes:
        - /var/filecache:/var/filecache
tempfiles:
    build: ../dataunity-tempfiles
    volumes:
        - /var/tempfiles:/var/tempfiles
pipesworker1:
    build: ../dataunity-pipes-worker
    links:
        - zookeeper
        - kafka
    volumes_from:
        - filecache
filemetadata:
    build: ../dataunity-datastructdef
    links:
        - zookeeper
        - kafka
    volumes_from:
        - filecache

# Real-time demo
# mosquitto:
#     image: toke/mosquitto:release-1.4.10-2
mosquitto:
    image: eclipse-mosquitto:1.4.10
    expose:
        - "1883"
# Web
web:
    build: ../dataunity-web
    # expose:
    #     - "5004"
    ports:
        # Pyramid
        - "0.0.0.0:6543:6543"
        # Aiohttp
        # - "0.0.0.0:8080:8080"
    environment:
        # ToDo: use container name as consumer group?
        #   Check container name is unique when scaling used.
        CONSUMER_GROUP: Web1
        PYTHONASYNCIODEBUG: 1
    links:
        - redis
        - fuseki
        - filemetadata
        - kafka
    volumes_from:
        - filecache
        - tempfiles
    # Development
    volumes:
        - ../dataunity-web:/code
    env_file: ./web_dev.env
    # TODO: Switch to 'exec' form so signals are passed to app process, not shell
    command: sh /code/startup_web.sh
    # command: pserve development.ini --reload
webasync:
    build: ../dataunity-web
    # expose:
    #     - "5004"
    ports:
        # Aiohttp
        - "0.0.0.0:8080:8080"
    environment:
        # ToDo: use container name as consumer group?
        #   Check container name is unique when scaling used.
        CONSUMER_GROUP: WebAsync
        PYTHONASYNCIODEBUG: 1
    links:
        - redis
        - fuseki
        - filemetadata
        - kafka
        - mosquitto
    volumes_from:
        - filecache
        - tempfiles
    # Development
    volumes:
        - ../dataunity-web:/code
    env_file: ./web_dev.env
    # TODO: Switch to 'exec' form so signals are passed to app process, not shell
    command: sh /code/startup.sh
    #command: gunicorn duweb.app:web_app --bind 0.0.0.0:8080 --worker-class aiohttp.worker.GunicornWebWorker --reload
# tmpfilemetadatamonitor:
#     build: ../kafka-topic-monitor
#     links:
#         - kafka
#         - zookeeper
#     environment:
#         CONSUMER_GROUP: FileMetaDataMonitor01
#         TOPIC: test1
# tmpfilemetadatareplymonitor:
#     build: ../kafka-topic-monitor
#     links:
#         - kafka
#         - zookeeper
#     environment:
#         CONSUMER_GROUP: FileMetaDataReplyMonitor01
#         TOPIC: test1reply
# tmppipesjobstartmonitor:
#     build: ../kafka-topic-monitor
#     links:
#         - kafka
#         - zookeeper
#     environment:
#         CONSUMER_GROUP: PipesJobStartMonitor01
#         TOPIC: dataworkflowjobstart
# tmppipesjobstatusmonitor:
#     build: ../kafka-topic-monitor
#     links:
#         - kafka
#         - zookeeper
#     environment:
#         CONSUMER_GROUP: PipesJobStatusMonitor01
#         TOPIC: dataworkflowjobstatus
