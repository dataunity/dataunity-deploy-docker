---
version: '2'
services:
    zookeeper:
        image: confluentinc/cp-zookeeper:3.3.0
        hostname: zookeeper
        ports:
            - "2181:2181"
        volumes:
            - ../volumes/zookeeper-data:/var/lib/zookeeper/data
            - ../volumes/zookeeper-transaction-logs:/var/lib/zookeeper/log
        environment:
            ZOOKEEPER_CLIENT_PORT: 2181
            ZOOKEEPER_TICK_TIME: 2000

    kafka:
        image: confluentinc/cp-kafka:3.3.0
        hostname: kafka
        depends_on:
            - zookeeper
        ports:
            - "9092:9092"
        volumes:
            - ../volumes/kafka-data:/var/lib/kafka/data
        environment:
            KAFKA_BROKER_ID: 1
            KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
            KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:9092'
            #KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
            KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
            #CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka:9092
            #CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:2181
            #CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
            #CONFLUENT_METRICS_ENABLE: 'true'
            #CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'

    connect:
        image: confluentinc/cp-kafka-connect:3.3.0
        hostname: connect
        depends_on:
            - zookeeper
            - kafka
            #- schema_registry
        #ports:
        #    - "8083:8083"
        env_file: ./connect_dev.env
        environment:
            CONNECT_BOOTSTRAP_SERVERS: 'kafka:9092'
            CONNECT_REST_ADVERTISED_HOST_NAME: connect
            CONNECT_REST_PORT: 8083
            CONNECT_GROUP_ID: compose-connect-group
            CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
            CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
            CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
            CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
            CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
            CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
            CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
            CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
            # CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: 'false'
            #CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema_registry:8081'
            CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
            #CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema_registry:8081'
            # Turn off schemas for non internal topics
            CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: 'false'
            CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: 'false'
            CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
            CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
            CONNECT_ZOOKEEPER_CONNECT: 'zookeeper:2181'

    fuseki:
        image: stain/jena-fuseki
        # network_mode: host
        ports:
            - "3030:3030"
        environment:
            ADMIN_PASSWORD: pw123
        volumes:
            - ../volumes/fuseki-data:/fuseki
        # ToDo: Create new Docker image which creates a directory for dataset under /fuseki (?)
        command: /jena-fuseki/fuseki-server --update --loc=/fuseki /dutmp

    redis:
        image: redis:2.8.9
        # hostname: redis
        # network_mode: host
        ports:
            - "6379:6379"

    filecache:
        build: ../dataunity-filecache
        volumes:
            - /var/filecache:/var/filecache

    tempfiles:
        build: ../dataunity-tempfiles
        volumes:
            - /var/tempfiles:/var/tempfiles

    pipesworker1:
        build: ../dataunity-pipes-worker
        depends_on:
            - zookeeper
            - kafka
        volumes_from:
            - filecache

    filemetadata:
        build: ../dataunity-datastructdef
        depends_on:
            - zookeeper
            - kafka
        volumes_from:
            - filecache

    # Real-time demo
    mosquitto:
        image: eclipse-mosquitto:1.4.10
        expose:
            - "1883"

    # Web
    web:
        build: ../dataunity-web
        environment:
            # ToDo: use container name as consumer group?
            #   Check container name is unique when scaling used.
            CONSUMER_GROUP: Web1
        links:
            - redis
            - fuseki
            - filemetadata
            - kafka
        volumes_from:
            - filecache
            - tempfiles
        env_file: ./web_dev.env
        # TODO: Switch to 'exec' form so signals are passed to app process, not shell
        command: sh /code/startup_web.sh
        # command: pserve development.ini --reload

    webasync:
        # TODO: use docker image rather then build directory (or github)
        build: ../du-web
        # network_mode: host
        environment:
            # ToDo: use container name as consumer group?
            #   Check container name is unique when scaling used.
            CONSUMER_GROUP: WebAsync
        depends_on:
            - redis
            - fuseki
            - filemetadata
            - kafka
            - connect
            - mosquitto
        volumes_from:
            - filecache
            - tempfiles
        # Development
        env_file: ./web_dev.env
        # TODO: Switch to 'exec' form so signals are passed to app process, not shell
        command: sh /code/startup.sh
        #command: gunicorn duweb.app:web_app --bind 0.0.0.0:8080 --worker-class aiohttp.worker.GunicornWebWorker --reload
